{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d20010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import getpass\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dbdb5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70012629-e5e7-426d-815d-d49db0e84cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 70\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_excel(\"responses.xlsx\")\n",
    "data = data[~data['remove_pii_5'].isna()]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(f\"Number of rows: {data.shape[0]}\")\n",
    "\n",
    "data = data.rename(columns={\n",
    "                        'remove_pii_1': 'pii_node_1',\n",
    "                        'remove_pii_2': 'pii_node_2',\n",
    "                        'remove_pii_3': 'pii_node_3',\n",
    "                        'remove_pii_4': 'pii_node_4',\n",
    "                        'remove_pii_5': 'pii_node_5',\n",
    "                        \"Symptoms, Reason to call, Rec\": \"get_symptoms\",\n",
    "                        \"Summarized\": \"summary\"\n",
    "                        })\n",
    "\n",
    "error_columns = ['pii_node_1', 'pii_node_2', 'pii_node_3', 'pii_node_4','pii_node_5','get_symptoms', 'summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bff37f8-c337-4866-96aa-f8c4067dc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[error_columns] = data[error_columns].astype(int).astype(bool)\n",
    "\n",
    "data[error_columns].head()\n",
    "\n",
    "data['no_errors'] = data[error_columns].all(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac2f1327",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "00711f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import operator\n",
    "from typing import Annotated, List, Optional, TypedDict, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(\"health-chats-graph\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    batch_id: str\n",
    "    input: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    output: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "df5924a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "llm = ChatAnthropic(model='claude-3-5-sonnet-20240620')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a2cf46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## PII ########\n",
    "# Patient Name, number, email\n",
    "# Patient ID\n",
    "# Birthdate\n",
    "# Medications or prescription\n",
    "# Policy number and insurance details\n",
    "# Symptoms\n",
    "# Summary\n",
    "#####################\n",
    "\n",
    "def check_remove_patient_name_number_email(state: State):\n",
    "    batch_id = state[\"batch_id\"][-1]\n",
    "    input = state['input'][-1]\n",
    "    output = state['output'][-1]\n",
    "\n",
    "    prompt = f\"\"\"You are a validator who is diligent and careful. When things are incorrect, you call it out and nothing gets past you.\n",
    "    Given a input and ouput, your goal is to check if the output followed the directions in the input.\n",
    "\n",
    "    Special instruction: If the input task was to remove something, but the input didn't have that text and the output was returned as is, please consider that a correct output.\n",
    "    Example: If the task is to remove a patient's name, but the input didn't mention the patient's name, the output wouldn't have removed the patient's name. In this case this is a correct output\n",
    "    and the reason is that the input didn't contain the specific text.\n",
    "\n",
    "    Analyze and output in JSON format with keys: \"reason\" (the reason why this is correct or incorrect), \"check\" (1 for correct and 0 for incorrect)\n",
    "\n",
    "    Input: {input}\n",
    "    Output: {output}\n",
    "\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"batch_id\": batch_id,\"input\": [input], \"output\":[output], \"messages\": [response.content]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "af042a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, Graph, START, END\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"check_remove_patient_name_number_email\", check_remove_patient_name_number_email)\n",
    "\n",
    "workflow.add_edge(START, \"check_remove_patient_name_number_email\")\n",
    "workflow.add_edge(\"check_remove_patient_name_number_email\", END)\n",
    "\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3cbfa0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "25b317f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of request tokens has exceeded your daily rate limit (https://docs.anthropic.com/en/api/rate-limits); see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m json_data\u001b[38;5;241m.\u001b[39mget(mapping[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m     output \u001b[38;5;241m=\u001b[39m json_data\u001b[38;5;241m.\u001b[39mget(mapping[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 37\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend(response)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/__init__.py:1281\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1283\u001b[0m     config,\n\u001b[1;32m   1284\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1285\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1286\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1287\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1288\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1290\u001b[0m ):\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1292\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/__init__.py:966\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[1;32m    965\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 966\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/__init__.py:1367\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step, timeout_exc_cls)\u001b[0m\n\u001b[1;32m   1365\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1366\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1367\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1372\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/executor.py:60\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m         \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/thread.py:52\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/retry.py:25\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     23\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py:2876\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2874\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   2875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2876\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2877\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2878\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/utils.py:102\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m    101\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 102\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[146], line 29\u001b[0m, in \u001b[0;36mcheck_remove_patient_name_number_email\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     14\u001b[0m output \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     16\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a validator who is diligent and careful. When things are incorrect, you call it out and nothing gets past you.\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124mGiven a input and ouput, your goal is to check if the output followed the directions in the input.\u001b[39m\n\u001b[1;32m     18\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 29\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_id,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28minput\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m:[output], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [response\u001b[38;5;241m.\u001b[39mcontent]}\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py:291\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    288\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    290\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 291\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    301\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py:791\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    785\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    789\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    790\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py:648\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    647\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 648\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    649\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    650\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    652\u001b[0m ]\n\u001b[1;32m    653\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py:638\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    637\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 638\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m         )\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py:860\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 860\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_anthropic/chat_models.py:752\u001b[0m, in \u001b[0;36mChatAnthropic._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m     stream_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[1;32m    749\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    750\u001b[0m     )\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[0;32m--> 752\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_output(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/anthropic/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/anthropic/resources/messages.py:860\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_given(timeout) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mtimeout \u001b[38;5;241m==\u001b[39m DEFAULT_TIMEOUT:\n\u001b[1;32m    859\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m\n\u001b[0;32m--> 860\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/messages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/anthropic/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/anthropic/_base_client.py:936\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    929\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    935\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/anthropic/_base_client.py:1025\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1024\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/anthropic/_base_client.py:1074\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1074\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/anthropic/_base_client.py:1025\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1024\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/anthropic/_base_client.py:1074\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1074\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/anthropic/_base_client.py:1040\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1039\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1043\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1044\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1048\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[1;32m   1049\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of request tokens has exceeded your daily rate limit (https://docs.anthropic.com/en/api/rate-limits); see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}"
     ]
    }
   ],
   "source": [
    "start = 36\n",
    "end = 70\n",
    "\n",
    "mappings = [\n",
    "    ('prompt-1', 'response-1'),\n",
    "    ('prompt-2', 'response-2'),\n",
    "    ('prompt-3', 'response-3'),\n",
    "    ('prompt-4', 'response-4'),\n",
    "    ('prompt-5', 'response-5'),\n",
    "    ('prompt-6', 'response-6'),\n",
    "    ('prompt-7', 'response-7'),\n",
    "    ]\n",
    "\n",
    "def write_to_file(batch_id, responses):    \n",
    "    previous_responses = pd.read_excel(\"llm-evals.xlsx\", sheet_name=\"Sheet1\")\n",
    "    checks = {f\"check-{idx+1}\": json.loads(r['messages'][0])['check'] for idx, r in enumerate(responses)}\n",
    "    reasons = {f\"reason-{idx+1}\": json.loads(r['messages'][0])['reason'] for idx, r in enumerate(responses)}\n",
    "    inputs = {f\"input-{idx+1}\": r[\"input\"][0] for idx, r in enumerate(responses)}\n",
    "    outputs = {f\"output-{idx+1}\": r[\"output\"][0] for idx, r in enumerate(responses)}\n",
    "\n",
    "\n",
    "    checks_pd = pd.DataFrame([{**inputs, **outputs, **checks, **reasons}])  \n",
    "    checks_pd['batch_id'] = batch_id      \n",
    "    \n",
    "    updated_responses = pd.concat([previous_responses, checks_pd])\n",
    "    updated_responses.to_excel(\"llm-evals.xlsx\", sheet_name=\"Sheet1\", index=False)\n",
    "\n",
    "for index, row in data[start:end].iterrows():\n",
    "    json_data = row.to_dict()\n",
    "    batch_id = json_data['batch_id']\n",
    "\n",
    "    responses = []\n",
    "    for mapping in mappings:\n",
    "        input = json_data.get(mapping[0], None)\n",
    "        output = json_data.get(mapping[1], None)\n",
    "\n",
    "        response = graph.invoke({\"batch_id\": [batch_id], \"input\": [input], \"output\": [output]})\n",
    "        responses.append(response)\n",
    "    \n",
    "    try:\n",
    "        write_to_file(batch_id, responses)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5873fc19",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2830615",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_outputs = pd.read_excel(\"llm-evals.xlsx\")\n",
    "llm_outputs = llm_outputs.rename(columns={\n",
    "                        'remove_pii_1': 'pii_node_1',\n",
    "                        'remove_pii_2': 'pii_node_2',\n",
    "                        'remove_pii_3': 'pii_node_3',\n",
    "                        'remove_pii_4': 'pii_node_4',\n",
    "                        'remove_pii_5': 'pii_node_5',\n",
    "                        \"Symptoms, Reason to call, Rec\": \"get_symptoms\",\n",
    "                        \"Summarized\": \"summary\"\n",
    "                        })\n",
    "\n",
    "error_columns = ['pii_node_1', 'pii_node_2', 'pii_node_3', 'pii_node_4','pii_node_5','get_symptoms', 'summary']\n",
    "\n",
    "llm_outputs[error_columns] = llm_outputs[error_columns].astype(int).astype(bool)\n",
    "\n",
    "llm_outputs[error_columns].head()\n",
    "\n",
    "llm_outputs['no_errors'] = llm_outputs[error_columns].all(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c568a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.parents = []\n",
    "\n",
    "    def add_parent(self, parent):\n",
    "        self.parents.append(parent)\n",
    "    \n",
    "def create_graph():\n",
    "    nodes = {\n",
    "        \"pii_node_1\": Node(\"pii_node_1\"),\n",
    "        \"pii_node_2\": Node(\"pii_node_2\"),\n",
    "        \"pii_node_3\": Node(\"pii_node_3\"),\n",
    "        \"pii_node_4\": Node(\"pii_node_4\"),\n",
    "        \"pii_node_5\": Node(\"pii_node_5\"),\n",
    "        \"get_symptoms\": Node(\"get_symptoms\"),\n",
    "        \"summary\": Node(\"summary\")\n",
    "    }\n",
    "    nodes['pii_node_2'].add_parent(nodes['pii_node_1'])\n",
    "    nodes['pii_node_3'].add_parent(nodes['pii_node_2'])\n",
    "    nodes['pii_node_4'].add_parent(nodes['pii_node_3'])\n",
    "    nodes['pii_node_5'].add_parent(nodes['pii_node_4'])\n",
    "    nodes['get_symptoms'].add_parent(nodes['pii_node_5'])\n",
    "    nodes['summary'].add_parent(nodes['get_symptoms'])\n",
    "\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ddf3792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = create_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3edd68e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing node: summary\n",
      "Overall failure probability for this node: 0.0000\n",
      "Independent failure probability: 0.0000\n",
      "Node failure because dep fails: 0\n",
      "Conditional failure probabilities given upstream dependency failures:\n",
      "  P(summary fails | get_symptoms fails): 0.0000\n",
      "\n",
      "Analyzing node: get_symptoms\n",
      "Overall failure probability for this node: 0.0000\n",
      "Independent failure probability: 0.0000\n",
      "Node failure because dep fails: 0\n",
      "Conditional failure probabilities given upstream dependency failures:\n",
      "  P(get_symptoms fails | pii_node_5 fails): 0.0000\n",
      "\n",
      "Analyzing node: pii_node_5\n",
      "Overall failure probability for this node: 0.0000\n",
      "Independent failure probability: 0.0000\n",
      "Node failure because dep fails: 0.0\n",
      "Conditional failure probabilities given upstream dependency failures:\n",
      "  P(pii_node_5 fails | pii_node_4 fails): 0.0000\n",
      "\n",
      "Analyzing node: pii_node_4\n",
      "Overall failure probability for this node: 0.0727\n",
      "Independent failure probability: 0.0727\n",
      "Node failure because dep fails: 0\n",
      "Conditional failure probabilities given upstream dependency failures:\n",
      "  P(pii_node_4 fails | pii_node_3 fails): 0.0000\n",
      "\n",
      "\n",
      "Root cause analysis complete.\n",
      "Debug path (from downstream to upstream): summary -> get_symptoms -> pii_node_5 -> pii_node_4\n",
      "Most likely root cause (most upstream issue): pii_node_4\n",
      "Independent failure probability of root cause: 0.0727\n",
      "Conditional failure probabilities given root cause's dependency failures:\n",
      "  P(pii_node_4 fails | pii_node_3 fails): 0.0000\n",
      "\n",
      "The most likely cause is an independent failure in node pii_node_4\n"
     ]
    }
   ],
   "source": [
    "# Rewritten code\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def calculate_probabilities(node: str, data: pd.DataFrame, dependencies: List[Node]) -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"Calculate failure probabilities for the node and its upstream dependencies.\"\"\"\n",
    "    node_fails = data[node] == False\n",
    "    p_node_fails = node_fails.mean()\n",
    "    \n",
    "    # Calculate independent failure probability\n",
    "    if not dependencies:\n",
    "        p_independent_fail = p_node_fails\n",
    "    else:\n",
    "        deps_pass = data[[dep.name for dep in dependencies]].all(axis=1)\n",
    "        p_independent_fail = (node_fails & deps_pass).sum() / deps_pass.sum()\n",
    "    \n",
    "    # Calculate conditional failure probabilities for dependencies\n",
    "    p_node_fails_given_dep_fails = {}\n",
    "    for dep in dependencies:\n",
    "        dep_fails = data[dep.name] == False\n",
    "\n",
    "        p_node_fails_given_dep_fails[dep.name] = (node_fails & dep_fails).sum() / dep_fails.sum() if dep_fails.sum() != 0 else 0\n",
    "\n",
    "        \n",
    "    return p_node_fails, p_independent_fail, p_node_fails_given_dep_fails\n",
    "\n",
    "def find_root_cause(node: str, data: pd.DataFrame, graph: Dict[str, Node]) -> Tuple[List[str], float, Dict[str, float]]:\n",
    "    \"\"\"Recursively find the root cause of failures, tracing from downstream to upstream.\"\"\"\n",
    "    dependencies = graph[node].parents  # These are upstream nodes\n",
    "\n",
    "    p_node_fails, p_independent_fail, p_node_fails_given_dep_fails = calculate_probabilities(node, data, dependencies)\n",
    "    \n",
    "    print(f\"Analyzing node: {node}\")\n",
    "    print(f\"Overall failure probability for this node: {p_node_fails:.4f}\")\n",
    "    print(f\"Independent failure probability: {p_independent_fail:.4f}\")\n",
    "    print(f\"Node failure because dep fails: {max([v for _, v in p_node_fails_given_dep_fails.items()])}\")\n",
    "    print(f\"Conditional failure probabilities given upstream dependency failures:\")\n",
    "    for dep, prob in p_node_fails_given_dep_fails.items():\n",
    "        print(f\"  P({node} fails | {dep} fails): {prob:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Check if independent failure is more likely than any upstream dependency failure\n",
    "    if p_independent_fail > max(p_node_fails_given_dep_fails.values(), default=0) :\n",
    "        return [node], p_independent_fail, p_node_fails_given_dep_fails\n",
    "\n",
    "    if not dependencies:\n",
    "        return [node], p_independent_fail, p_node_fails_given_dep_fails\n",
    "\n",
    "    max_dep = max(p_node_fails_given_dep_fails, key=p_node_fails_given_dep_fails.get)\n",
    "    upstream_path, upstream_independent_prob, upstream_final_probs = find_root_cause(max_dep, data, graph)\n",
    "    \n",
    "    return [node] + upstream_path, upstream_independent_prob, upstream_final_probs\n",
    "\n",
    "def improve_system(downstream_node: str, data: pd.DataFrame, graph: Dict[str, Node]) -> Tuple[List[str], float, Dict[str, float]]:\n",
    "    \"\"\"Entry point for the root cause analysis, starting from the most downstream node.\"\"\"\n",
    "    path, independent_prob, final_probs = find_root_cause(downstream_node, data, graph)\n",
    "    \n",
    "    print(\"\\nRoot cause analysis complete.\")\n",
    "    print(f\"Debug path (from downstream to upstream): {' -> '.join(path)}\")\n",
    "    print(f\"Most likely root cause (most upstream issue): {path[-1]}\")\n",
    "    print(f\"Independent failure probability of root cause: {independent_prob:.4f}\")\n",
    "    print(\"Conditional failure probabilities given root cause's dependency failures:\")\n",
    "    for dep, prob in final_probs.items():\n",
    "        print(f\"  P({path[-1]} fails | {dep} fails): {prob:.4f}\")\n",
    "    \n",
    "    if independent_prob > max(final_probs.values(), default=0):\n",
    "        print()\n",
    "        print(f\"The most likely cause is an independent failure in node {path[-1]}\")\n",
    "    else:\n",
    "        most_likely_dep = max(final_probs.values(), default=0)\n",
    "        print()\n",
    "        print(f\"The most likely cause is a failure in dependency: {most_likely_dep}\")\n",
    "    \n",
    "    return path, independent_prob, final_probs\n",
    "\n",
    "output = improve_system('summary', data=llm_outputs, graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d24abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
